# Word Embeddings with Word2Vec

This repository contains a Python script that demonstrates how to train a Word2Vec model on text data and visualize word embeddings in a 2D space using Principal Component Analysis (PCA).

## Overview
Word embeddings are dense vector representations of words in a continuous vector space, learned from large text corpora using neural network-based language models like Word2Vec. This project showcases the process of training a Word2Vec model and visualizing word embeddings for exploratory analysis.

## Requirements
* Python 3.x
* gensim library for Word2Vec modeling
* nltk library for accessing the Brown corpus
* matplotlib library for data visualization

## Install the required Python libraries
~~~
pip install gensim nltk matplotlib
~~~
