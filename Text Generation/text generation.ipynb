{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you have the transformers library installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 20.5/134.8 kB 217.9 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/134.8 kB 281.8 kB/s eta 0:00:01\n",
      "     -------------- ---------------------- 51.2/134.8 kB 327.7 kB/s eta 0:00:01\n",
      "     -----------------------------------  133.1/134.8 kB 605.3 kB/s eta 0:00:01\n",
      "     -----------------------------------  133.1/134.8 kB 605.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 134.8/134.8 kB 468.4 kB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikhi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "   ---------------------------------------- 0.0/8.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.8 MB 991.0 kB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.1/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.2/8.8 MB 1.2 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/8.8 MB 1.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/8.8 MB 1.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.7/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/8.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.9/8.8 MB 1.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/8.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/8.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.1/8.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.3/8.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.3/8.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.5/8.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.6/8.8 MB 1.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.6/8.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.7/8.8 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.8/8.8 MB 1.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.0/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.1/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.3/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.4/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.5/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.5/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.7/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.7/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.7/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.8/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.9/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.0/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.1/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.2/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.2/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.2/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.4/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.5/8.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.7/8.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.0/8.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.3/8.8 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.5/8.8 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.8/8.8 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.0/8.8 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.0/8.8 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.1/8.8 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.4/8.8 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.9/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.2/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.4/8.8 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.7/8.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.0/8.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.2/8.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.5/8.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.9/8.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.2/8.8 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.5/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.8/8.8 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 112.6/388.9 kB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 378.9/388.9 kB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 388.9/388.9 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.7 kB ? eta -:--:--\n",
      "   -------------------------------------- - 133.1/138.7 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 138.7/138.7 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp312-none-win_amd64.whl (270 kB)\n",
      "   ---------------------------------------- 0.0/270.7 kB ? eta -:--:--\n",
      "   ---------------------------------------  266.2/270.7 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/270.7 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/270.7 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/270.7 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 270.7/270.7 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.2 MB 6.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   -------------------------------------- - 163.8/172.0 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 172.0/172.0 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 pyyaml-6.0.1 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_gpt2(prompt, max_length=100, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate text using the GPT-2 language model based on a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text prompt to generate continuation from.\n",
    "        max_length (int): The maximum length of the generated text.\n",
    "        num_return_sequences (int): Number of different sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text based on the prompt.\n",
    "    \"\"\"\n",
    "    # Define the pre-trained GPT-2 model and tokenizer\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Encode the prompt text using the GPT-2 tokenizer\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text based on the encoded prompt using the GPT-2 model\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated output to text, skipping special tokens\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nikhi\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
      "\n",
      "The world that was created was not the same as the one that is now. It was an endless, endless world. And the Gods were not born of nothing. They were created of a single, single thing. That was why the universe was so beautiful. Because the cosmos was made of two\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt for generating text\n",
    "    prompt_text = \"Once upon a time\"\n",
    "\n",
    "    # Generate text based on the prompt using GPT-2\n",
    "    generated_text = generate_text_with_gpt2(prompt_text)\n",
    "\n",
    "    # Print the generated text\n",
    "    print(\"Generated Text:\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
